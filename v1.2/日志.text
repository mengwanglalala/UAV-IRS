10.21
调试出来了state=state的情况，reward曲线正常，证明了DDPG有效
在设置state=nextstate的情况时，发现reward会出现断层的情况（如图8000），推测是因为replay-buffer设置太大，之前的旧记忆一直没
清除，所以之后在随机选择的时候选择到reward低的情况，导致reward出现断层，突然为0并难以在一段时间回到良好状态.根据图像，设置为2000、

设置为2000时发现还是会出现突然为0的情况，问题在于cvx求解时出现的无解情况，此时返回的rewrd直接为0，（这种应该只是偶然发生
不会持续太久），不过reward上升的效果比之前更好
问题代码：
    try:
        eigval, eigvec = np.linalg.eig(X.value) #极个别情况会出现prob无解的情况，为了防止在迭代意外终止加此项
    except:
        return 0, state_next, 1
目前还没有仔细寻找bug

下一步计划：
设置
UAV移动约束（目前设置的是UAV对任意移动方向距离不超过1，使用tanh默认单步距离约束）
UAV能量约束（doing）
优化Wp主动波束形成

之后
扩展多用户（可能会导致theta优化不可用，但是可以利用一段时间只为信号最强服务，并设置公平系数，保证每个user都被顾及到）
考虑给user传递能量
完善应用情况


10.22
在调整方差var_a后发现，设置为1.5会导致reward持续下降（可能是因为方差太大导致采集到的学习样本很少有正面样本）
当设置为0.5时训练过程中出现的reward突然下降会导致后续很难再很好的继续学习，甚至曲线不再产生波动

故还是需要找到是什么原因导致reward突然下降，目前认为的可能是解算theta时产生的问题
并不是结算theta的

step=20，var_a=1是效果最好的

reward在低位持续的主要原因在于训练时state的偏差过大，如果一开始比较幸运的找到合适位置/方向，那么曲线相对会更快收敛并到达最优值
如果训练过程中走反了，agent会很难调整回来

由此加入无人机飞行边界限制，减少了训练到后期reward起不来的情况

重大发现！计算的reward图之所以会出现突然下降的情况是因为无人机高度设置的问题
    if (state_next[2] < 0)|(state_next[2] > 20):
        area_flag=1
由于计算时action[2]的参数是tanh范围为【-1，1】如果第三项持续选择为负值会导致reward猛降。
如果固定高度则不容易出现猛降的情况，但是不及时终止训练或者方差给的太小会掉入局部最优或者直接跑飞

不仅仅是无人机高度设置的问题，原本reward设置如下
 area_flag = 0
    if (state_next[0] < 0 ) | (state_next[0] > 20):
        area_flag = 1
    if (state_next[1] < -10) | (state_next[1] > 10):
        area_flag = 1
    if (state_next[2] > 20):
        area_flag=1
    if area_flag:
        return 0, state, done
没有对出范围部分进行惩罚（show_result文件目前不支持负数。0也可，但是负数效果更好）

最最重要的原因是因为高奖励区域是一个范围很小的区域，而其它位置要明显低于这个小区域，就像倒立摆一样，在微调的时候
很容易掉下来，而这一掉就可能会出现agent走入死胡同的情况
snr_min = snr_min_ * snr_min_
奖励是一个二次项
如果改为一次项表现的会很稳定，但是原本的var_a参数不够用了，容易陷在局部最优解，调至1.5表现尚可


下一步计划：
设置
UAV移动约束（增加了无人机飞行区域限制）
UAV能量约束（doing）
优化Wp主动波束形成

之后
扩展多用户（可能会导致theta优化不可用，但是可以利用一段时间只为信号最强服务，并设置公平系数，保证每个user都被顾及到）
考虑给user传递能量
完善应用情况